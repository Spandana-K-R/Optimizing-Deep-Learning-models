{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST classifier model\n",
    "In this notebook, I have demonstrated the usage of Quantization tools in Pytorch.  \n",
    "We shall start by defining a simple Neural Network model for classification and then we will apply three methods of Quantization and compare the accuracy, model size and inference runtime against the original model.  \n",
    "The three methods are:\n",
    "1. Dynamic Quantization\n",
    "2. Post-training Static Quantization\n",
    "3. Quantized Aware Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size   = 128\n",
    "valid_size   = 0.2\n",
    "epochs       = 5\n",
    "transform    = transforms.ToTensor()  # Convert image to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the Train and Test set\n",
    "\n",
    "trainset = datasets.FashionMNIST('.', train=True, download=True, transform=transform)\n",
    "testset = datasets.FashionMNIST('.', train=False, download=True, transform=transform)\n",
    "\n",
    "print(\"Length of the train set\", len(trainset))\n",
    "print(\"Length of the test set\", len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample image from the train set and the label\n",
    "\n",
    "img_num = 10\n",
    "print(\"Label:\", trainset[img_num][1])\n",
    "plt.imshow(trainset[img_num][0][0,:,:],'gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set indices into training and validation set indices using 80:20 ratio\n",
    "\n",
    "val_size   = int(len(trainset) * valid_size)\n",
    "train_size = int(len(trainset) * (1-valid_size))\n",
    "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for training, validation and testing datasets\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader  = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "\n",
    "class FMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FMNIST,self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 1500, bias=False)\n",
    "        self.fc2 = nn.Linear(1500, 750, bias=False)\n",
    "        self.fc3 = nn.Linear(750, 300, bias=False)\n",
    "        self.fc4 = nn.Linear(300, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)#F.log_softmax(, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "model = FMNIST()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Loss function and the Optimization function \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training and Validation\n",
    "\n",
    "def train_and_val(model, trainloader, criterion, optimizer, epochs):\n",
    "    min_val_loss = np.Inf\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        \n",
    "        # TRAINING\n",
    "        model.train()                                           \n",
    "        for image, label in trainloader:\n",
    "            image, label = image, label\n",
    "            # Set gradients to zero\n",
    "            optimizer.zero_grad()          \n",
    "            output = model(image)\n",
    "            # Calculate loss according to CrossEntropy\n",
    "            loss   = criterion(output, label)                   \n",
    "            # Backward propagation of loss\n",
    "            loss.backward()                                     \n",
    "            # Run Adam optimiser\n",
    "            optimizer.step()                                    \n",
    "            # Set train loss as the accumulated sum of loss times the batch size\n",
    "            train_loss += loss.item()                           \n",
    "\n",
    "        # VALIDATION\n",
    "        model.eval()                                            \n",
    "        for image, label in validloader:\n",
    "            image, label = image, label\n",
    "            output = model(image)\n",
    "            loss   = criterion(output, label)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "        train_loss = (train_loss*batch_size)/train_size\n",
    "        val_loss   = (valid_loss*batch_size)/val_size\n",
    "\n",
    "        print('Epoch [{}/{}]: \\tTraining Loss: {:.5f} \\tValidation Loss: {:.5f}'.format(\n",
    "          epoch+1,\n",
    "          epochs,\n",
    "          train_loss,\n",
    "          val_loss\n",
    "          ))\n",
    "        \n",
    "        # Save the model only if val loss has decreased\n",
    "        if val_loss <= min_val_loss:                            \n",
    "            print('Validation loss has decreased ({:.5f} --> {:.5f}).  Model saved!'.format(\n",
    "            min_val_loss,\n",
    "            val_loss))\n",
    "            torch.save(model.state_dict(), 'model_q.pt')\n",
    "            min_val_loss = val_loss\n",
    "            \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy - testing\n",
    "\n",
    "def test_accuracy(model, testloader, criterion):\n",
    "    test_loss     = 0.0\n",
    "    class_correct = list(0. for i in range(10))  # 10 classes                          \n",
    "    class_total   = list(0. for i in range(10))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for image, target in testloader:\n",
    "        image, target = image, target\n",
    "        output = model(image)\n",
    "        loss   = criterion(output, target)\n",
    "        test_loss += loss.item()*image.size(0)\n",
    "        # Get argmax\n",
    "        _, pred = torch.max(F.softmax(output, dim=1), 1)   \n",
    "        # Array of 1's and 0's\n",
    "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))           \n",
    "        \n",
    "        for i in range(len(target)):\n",
    "            label = target.data[i]\n",
    "            # Increment if pred==target\n",
    "            class_correct[label] += correct[i].item()\n",
    "            class_total[label]   += 1\n",
    "            \n",
    "    # Percentage\n",
    "    overall_accuracy = 100. * np.sum(class_correct) / np.sum(class_total)  \n",
    "    return overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will run the training function and the test accuracy\n",
    "\n",
    "def main(model,epochs=epochs):\n",
    "    train_and_val(model,trainloader, criterion, optimizer, epochs)\n",
    "    \n",
    "    model.load_state_dict(torch.load('model_q.pt'))\n",
    "    \n",
    "    accuracy = test_accuracy(model, testloader, criterion)\n",
    "    \n",
    "    return model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training and validation here or load the saved model like in the next code block\n",
    "\n",
    "model, model_accuracy = main(model)\n",
    "print(model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you want to load the model again\n",
    "\n",
    "# model.load_state_dict(torch.load('model_q.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to print the size of the model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "print_size_of_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Quantization  \n",
    "Now that the original FMNIST model has been defined and trained, we can specify which layers we want to quantize. Here, we will specify nn.Linear layers for quatization but, in our model, we only have nn.Linear layers so all the layers get quantized. If the model had CNN layers then those layers would not get quantized.<br>\n",
    "We also specify that we want weights to be converted to int8 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "\n",
    "dynamic_quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_size_of_model(dynamic_quantized_model)\n",
    "print_size_of_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the performance of the dynamic quantized model against the original model\n",
    "\n",
    "dynamic_quantized_accuracy = test_accuracy(dynamic_quantized_model, testloader, criterion)\n",
    "print(\"Accuracy of the dynamic quantized model: {}%\".format(dynamic_quantized_accuracy))\n",
    "\n",
    "original_accuracy          = test_accuracy(model, testloader, criterion)\n",
    "print(\"Accuracy of the original model         : {}%\".format(original_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-training Static Quantization\n",
    "Statis quantization works by installing observers (for example: *MinMaxOberserver*, *HistogramObserver*, and others) which observe the model from input to the output and collect statistics required to later quantize the parameters. Once the model is prepared this way, we can convert the model (i.e. float32 weights become int8) using an appropriate quantization technique to get the quantized model. And, as expected the size of the model reduces. But, it is not possible to perform inference at this stage as the inputs are still the same and have not been adapted to the quantized model. So the model class needs to be defined again (there is no work around for this problem) with Quantstubs and Dequantstubs. <br>\n",
    "\n",
    "For more details, please go through \"Understanding_quantization.pdf\" available in this repository. I found this PDF [here](https://github.com/pytorch/pytorch/issues/18318)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the model architecture\n",
    "\n",
    "from torch.quantization import QConfig, MinMaxObserver, HistogramObserver, default_observer, \\\n",
    "default_per_channel_weight_observer\n",
    "\n",
    "class LinearReLU(nn.Sequential):\n",
    "    def __init__(self,in_channel, out_channel):\n",
    "        super(LinearReLU,self).__init__(nn.Linear(in_channel, out_channel, bias=False), nn.ReLU())\n",
    "\n",
    "class FMNIST_quant(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FMNIST_quant,self).__init__()\n",
    "        my_qconfig    = QConfig(activation=MinMaxObserver.with_args(dtype=torch.quint8),\n",
    "                                weight=default_per_channel_weight_observer.with_args(dtype=torch.qint8))\n",
    "        \n",
    "        self.quant    = torch.quantization.QuantStub(my_qconfig)\n",
    "        self.sq1      = LinearReLU(784,1500)\n",
    "        \n",
    "        self.sq2      = LinearReLU(1500,750)\n",
    "        \n",
    "        self.sq3      = LinearReLU(750,300)\n",
    "        \n",
    "        self.fc_out   = nn.Linear(300, 10, bias=False)\n",
    "        \n",
    "        self.dequant  = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.quant(x)\n",
    "        \n",
    "        x = F.relu(self.sq1(x))\n",
    "        x = F.relu(self.sq2(x))\n",
    "        x = F.relu(self.sq3(x))\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        x = self.dequant(x)\n",
    "        \n",
    "        # No need for F.log_softmax() as Cross Entropy loss does that implicitly\n",
    "        return x\n",
    "    \n",
    "    def fuse_model(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) == LinearReLU:\n",
    "                torch.quantization.fuse_modules(m, ['0', '1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to just implement the forward pass for observer calibration\n",
    "\n",
    "def calibrate(model, criterion, data_loader, n_eval=150):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            count += 1\n",
    "            if count >= n_eval:\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with the observers in place\n",
    "\n",
    "model_quant = FMNIST_quant()\n",
    "\n",
    "# Defining the Loss function and the Optimization function \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_quant.parameters(), lr=0.01)\n",
    "\n",
    "model_quant, model_quant_accuracy = main(model_quant)\n",
    "print(model_quant_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the model\n",
    "\n",
    "# model_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_with_stubs.load_state_dict(torch.load('model_q.pt'))\n",
    "model_quant.eval()\n",
    "\n",
    "model_quant.fuse_model()\n",
    "\n",
    "# Set the qconfig - fbgemm for x86 and qnnpack for ARM qnnpack\n",
    "model_quant.qconfig = QConfig(activation=MinMaxObserver.with_args(dtype=torch.quint8),\n",
    "                        weight=default_per_channel_weight_observer.with_args(dtype=torch.qint8))\n",
    "print(model_quant.qconfig)\n",
    "torch.backends.quantized.engine = 'fbgemm'\n",
    "\n",
    "# Insert observers, calibrate the model and collect statistics\n",
    "torch.quantization.prepare(model_quant, inplace=True)\n",
    "\n",
    "calibrate(model_quant,criterion,testloader)\n",
    "\n",
    "# Convert to the quantized version\n",
    "torch.quantization.convert(model_quant, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the model size\n",
    "\n",
    "print_size_of_model(model_quant)  # quantized model\n",
    "print_size_of_model(model)        # original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the test accuracies\n",
    "\n",
    "static_quantized_accuracy = test_accuracy(model_quant, testloader, criterion)\n",
    "\n",
    "print(\"Accuracy of the static quantized model : {}%\".format(static_quantized_accuracy))\n",
    "print(\"Accuracy of the original model         : {}%\".format(model_quant_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the quantized weights\n",
    "\n",
    "model_quant.sq1[0].weight().int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the stat_dict of the model\n",
    "\n",
    "# for key,val in model_quant.state_dict().items():\n",
    "#     print(key)\n",
    "#     print(val)\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization Aware Training\n",
    "\n",
    "Quantization Aware Training (QAT) is another way of optimizing Deep Learning models by fake quantizing the weights and activations during training. The forward and backward process of training is modified to use quantized values while still keep them as float variables. Once the model accuracy is satisfactory, we can then convert the float variables to int8 variables and reduce the size of the model without compromising on the accuracy. This method typically yields higher accuracy in comparison to the other methods discussed earlier.  \n",
    "\n",
    "The model architecture remains the same as Static Quantization (FMNIST_quant) but qconfig has to be defined differently. I am redefining the function train_and_val as train_one_epoch and also the function main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training one epoch at a time\n",
    "\n",
    "def train_one_epoch(model, trainloader, criterion, optimizer):\n",
    "    \n",
    "    model.train() \n",
    "    \n",
    "    for image, label in trainloader:\n",
    "        image, label = image, label\n",
    "        optimizer.zero_grad()                           \n",
    "        output = model(image)\n",
    "        loss   = criterion(output, label)               \n",
    "        loss.backward()                                 \n",
    "        optimizer.step()                          \n",
    "\n",
    "    return(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will run the training function and the test accuracy\n",
    "\n",
    "def main(model,epochs=epochs):\n",
    "\n",
    "    for e in range(epochs):\n",
    "        e_loss = train_one_epoch(model,trainloader, criterion, optimizer)\n",
    "        \n",
    "        # Freeze quantizer parameters (scale and zero points)\n",
    "        if e>4:\n",
    "            model.apply(torch.quantization.disable_observer)\n",
    "        \n",
    "        quantized_model = torch.quantization.convert(model.eval(), inplace=False)\n",
    "        quantized_model.eval()\n",
    "\n",
    "        acc    = test_accuracy(quantized_model, testloader, criterion)\n",
    "        print(\"Epoch [{}/{}]: \\tTraining loss: {:2.3f} \\tTest accuracy: {:2.3f}%\".format(e+1,epochs,e_loss,acc))\n",
    "    \n",
    "    return quantized_model, acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "\n",
    "qat_model = FMNIST_quant()\n",
    "qat_model\n",
    "\n",
    "# Defining the Loss function and the Optimization function \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(qat_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we will fuse the model layers as before, specify the quantization configuration and \n",
    "# then prepare the fake quants layers\n",
    "\n",
    "qat_model.fuse_model()\n",
    "\n",
    "qat_model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "\n",
    "# OR if you want to explicitly type in the qconfig instead of using get_default_qat_qconfig\n",
    "# from torch.quantization import FakeQuantize, default_per_channel_weight_fake_quant\n",
    "# qat_model.qconfig = QConfig(activation=FakeQuantize.with_args(observer=MovingAverageMinMaxObserver,\n",
    "#                                                               quant_min=0,\n",
    "#                                                               quant_max=255,\n",
    "#                                                               reduce_range=True),\n",
    "#                             weight=default_per_channel_weight_fake_quant)\n",
    "\n",
    "# Insert fake quantization modules into the model\n",
    "torch.quantization.prepare_qat(qat_model, inplace=True)\n",
    "\n",
    "print('After preparation for QAT, note the fake-quantization modules \\n',qat_model.sq1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform QAT and check the accuracy of the model after quantization\n",
    "\n",
    "qat_model, qat_model_accuracy = main(qat_model,5)\n",
    "print(qat_model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the quantized model is smaller in size compared to the original model\n",
    "\n",
    "print_size_of_model(qat_model)    # QAT model\n",
    "print_size_of_model(model)        # original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference time\n",
    "Here, we shall compare evaluate the original and the quantized model for the time they take to perform inference on a 2048 images.  \n",
    "\n",
    "Firstly, we will save the models using torchscript (serialize the model) and then we will load the models to evaluate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the models\n",
    "\n",
    "script_qat_model = torch.jit.script(qat_model)\n",
    "print(\"Pythonic code of script qat_model:\\n\", script_qat_model.code)\n",
    "script_qat_model.save('qat_model.pt')\n",
    "\n",
    "script_original_model = torch.jit.script(model)\n",
    "print(\"Pythonic code of script model:\\n\",script_original_model.code)\n",
    "script_original_model.save('original_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_benchmark(model_file, img_loader):\n",
    "    elapsed = 0\n",
    "    model = torch.jit.load(model_file)\n",
    "    model.eval()\n",
    "    num_batches = 16\n",
    "    \n",
    "    # Run the scripted model on a few batches of images\n",
    "    for i, (images, target) in enumerate(img_loader):\n",
    "        if i < num_batches:\n",
    "            start = time.time()\n",
    "            output = model(images)\n",
    "            end = time.time()\n",
    "            elapsed = elapsed + (end-start)\n",
    "        else:\n",
    "            break\n",
    "    num_images = images.size()[0] * num_batches\n",
    "\n",
    "    print('Elapsed time using model {} for {} images: {:0.3f} ms'.format(model_file,\\\n",
    "                                                                         num_images,\\\n",
    "                                                                         elapsed/num_images*1000))\n",
    "\n",
    "# Run the benchmark for both the models\n",
    "run_benchmark('qat_model.pt', testloader)\n",
    "run_benchmark('original_model.pt', testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
