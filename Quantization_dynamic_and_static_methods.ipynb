{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fashion MNIST classifier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size   = 128\n",
    "valid_size   = 0.2\n",
    "epochs       = 5\n",
    "transform    = transforms.ToTensor()  # Convert image to Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train set 60000\n",
      "Length of the test set 10000\n"
     ]
    }
   ],
   "source": [
    "# Download the Train and Test set\n",
    "\n",
    "trainset = datasets.FashionMNIST('.', train=True, download=True, transform=transform)\n",
    "testset = datasets.FashionMNIST('.', train=False, download=True, transform=transform)\n",
    "\n",
    "print(\"Length of the train set\", len(trainset))\n",
    "print(\"Length of the test set\", len(testset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sample image from the train set and the label\n",
    "\n",
    "img_num = 10\n",
    "print(\"Label:\", trainset[img_num][1])\n",
    "plt.imshow(trainset[img_num][0][0,:,:],'gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the training set indices into training and validation set indices using 80:20 ratio\n",
    "\n",
    "val_size   = int(len(trainset) * valid_size)\n",
    "train_size = int(len(trainset) * (1-valid_size))\n",
    "train_dataset, val_dataset = random_split(trainset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataloaders for training, validation and testing datasets\n",
    "\n",
    "trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "validloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "testloader  = DataLoader(testset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture\n",
    "\n",
    "class FMNIST(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FMNIST,self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 1500, bias=False)\n",
    "        self.fc2 = nn.Linear(1500, 750, bias=False)\n",
    "        self.fc3 = nn.Linear(750, 300, bias=False)\n",
    "        self.fc4 = nn.Linear(300, 10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)#F.log_softmax(, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FMNIST(\n",
       "  (fc1): Linear(in_features=784, out_features=1500, bias=False)\n",
       "  (fc2): Linear(in_features=1500, out_features=750, bias=False)\n",
       "  (fc3): Linear(in_features=750, out_features=300, bias=False)\n",
       "  (fc4): Linear(in_features=300, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create model\n",
    "\n",
    "model = FMNIST()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Loss function and the Optimization function \n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training and Validation\n",
    "\n",
    "def train_and_val(model, trainloader, criterion, optimizer, epochs):\n",
    "    min_val_loss = np.Inf\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "\n",
    "        model.train()                                           # TRAINING\n",
    "        for image, label in trainloader:\n",
    "            image, label = image, label\n",
    "            optimizer.zero_grad()                               # Set Gradients to zero\n",
    "            output = model(image)\n",
    "            loss   = criterion(output, label)                   # Calculate loss according to CrossEntropy\n",
    "            loss.backward()                                     # Backward propagation of loss\n",
    "            optimizer.step()                                    # Run Adam optimiser\n",
    "            train_loss += loss.item()                           # Set train loss as the accumulated sum of loss times the batch size\n",
    "\n",
    "        \n",
    "#         for param in model.parameters():\n",
    "#             print(param.data)\n",
    "        \n",
    "        model.eval()                                            # VALIDATION\n",
    "        for image, label in validloader:\n",
    "            image, label = image, label\n",
    "            output = model(image)\n",
    "            loss   = criterion(output, label)\n",
    "            valid_loss += loss.item()\n",
    "\n",
    "        train_loss = (train_loss*batch_size)/train_size\n",
    "        val_loss   = (valid_loss*batch_size)/val_size\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.5f} \\tValidation Loss: {:.5f}'.format(\n",
    "          epoch+1, \n",
    "          train_loss,\n",
    "          val_loss\n",
    "          ))\n",
    "\n",
    "        if val_loss <= min_val_loss:                            # Save the model only if val loss has decreased\n",
    "            print('Validation loss has decreased ({:.5f} --> {:.5f}).  Model saved!'.format(\n",
    "            min_val_loss,\n",
    "            val_loss))\n",
    "            torch.save(model.state_dict(), 'model_q.pt')\n",
    "            min_val_loss = val_loss\n",
    "            \n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check accuracy - testing\n",
    "\n",
    "def test_accuracy(model, testloader, criterion):\n",
    "    test_loss     = 0.0\n",
    "    class_correct = list(0. for i in range(10))                            # 10 classes\n",
    "    class_total   = list(0. for i in range(10))\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    for image, target in testloader:\n",
    "        image, target = image, target\n",
    "        output = model(image)\n",
    "        loss   = criterion(output, target)\n",
    "        test_loss += loss.item()*image.size(0)\n",
    "        _, pred = torch.max(F.softmax(output, dim=1), 1)                   # Get argmax\n",
    "        correct = np.squeeze(pred.eq(target.data.view_as(pred)))           # Array of 1's and 0's\n",
    "        \n",
    "        for i in range(len(target)):\n",
    "            label = target.data[i]\n",
    "            class_correct[label] += correct[i].item()                      # Increment if pred==target\n",
    "            class_total[label]   += 1\n",
    "            \n",
    "    overall_accuracy = 100. * np.sum(class_correct) / np.sum(class_total)  # Percentage\n",
    "    return overall_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will run the training function and the test accuracy\n",
    "\n",
    "def main(model,epochs=epochs):\n",
    "    train_and_val(model,trainloader, criterion, optimizer, epochs)\n",
    "    \n",
    "    model.load_state_dict(torch.load('model_q.pt'))\n",
    "    \n",
    "    accuracy = test_accuracy(model, testloader, criterion)\n",
    "    \n",
    "    return model, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.67668 \tValidation Loss: 0.48468\n",
      "Validation loss has decreased (inf --> 0.48468).  Model saved!\n",
      "\n",
      "Epoch: 2 \tTraining Loss: 0.45694 \tValidation Loss: 0.43210\n",
      "Validation loss has decreased (0.48468 --> 0.43210).  Model saved!\n",
      "\n",
      "Epoch: 3 \tTraining Loss: 0.41891 \tValidation Loss: 0.41219\n",
      "Validation loss has decreased (0.43210 --> 0.41219).  Model saved!\n",
      "\n",
      "Epoch: 4 \tTraining Loss: 0.40419 \tValidation Loss: 0.42011\n",
      "\n",
      "Epoch: 5 \tTraining Loss: 0.38635 \tValidation Loss: 0.40298\n",
      "Validation loss has decreased (0.41219 --> 0.40298).  Model saved!\n",
      "\n",
      "84.77\n"
     ]
    }
   ],
   "source": [
    "# Run training and validation here or load the saved model like in the next code block\n",
    "\n",
    "model, model_accuracy = main(model)\n",
    "print(model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you want to load the model again\n",
    "\n",
    "# model.load_state_dict(torch.load('model_q.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 10.116807\n"
     ]
    }
   ],
   "source": [
    "# Function to print the size of the model\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "print_size_of_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Quantization  \n",
    "Now that the original FMNIST model has been defined and trained, we can specify which layers we want to quantize. Here, we will specify nn.Linear layers for quatization but, in our model, we only have nn.Linear layers so all the layers get quantized. If the model had CNN layers then those layers would not get quantized.<br>\n",
    "We also specify that we want weights to be converted to int8 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization\n",
    "\n",
    "dynamic_quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 2.531199\n",
      "Size (MB): 10.116807\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(dynamic_quantized_model)\n",
    "print_size_of_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the dynamic quantized model: 84.75%\n",
      "Accuracy of the original model         : 84.77%\n"
     ]
    }
   ],
   "source": [
    "# Compare the performance of the dynamic quantized model against the original model\n",
    "\n",
    "dynamic_quantized_accuracy = test_accuracy(dynamic_quantized_model, testloader, criterion)\n",
    "print(\"Accuracy of the dynamic quantized model: {}%\".format(dynamic_quantized_accuracy))\n",
    "\n",
    "original_accuracy          = test_accuracy(model, testloader, criterion)\n",
    "print(\"Accuracy of the original model         : {}%\".format(original_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-training Static Quantization\n",
    "Statis quantization works by installing observers (for example: *MinMaxOberserver*, *HistogramObserver*, and others) which observe the model from input to the output and collect statistics required to later quantize the parameters. Once the model is prepared this way, we can convert the model (i.e. float32 weights become int8) using an appropriate quantization technique to get the quantized model. And, as expected the size of the model reduces. But, it is not possible to perform inference at this stage as the inputs are still the same and have not been adapted to the quantized model. So the model class needs to be defined again (there is no work around for this problem) with Quantstubs and Dequantstubs. <br>\n",
    "\n",
    "For more details, please go through \"Understanding_quantization.pdf\" available in this repository. I found this PDF [here](https://github.com/pytorch/pytorch/issues/18318)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redefine the model architecture\n",
    "\n",
    "from torch.quantization import QConfig, MinMaxObserver, HistogramObserver, default_observer, default_per_channel_weight_observer\n",
    "\n",
    "class LinearReLU(nn.Sequential):\n",
    "    def __init__(self,in_channel, out_channel):\n",
    "        super(LinearReLU,self).__init__(nn.Linear(in_channel, out_channel, bias=False), nn.ReLU())\n",
    "\n",
    "class FMNIST_quant(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FMNIST_quant,self).__init__()\n",
    "        my_qconfig    = QConfig(activation=MinMaxObserver.with_args(dtype=torch.quint8),\n",
    "                                weight=default_per_channel_weight_observer.with_args(dtype=torch.qint8))\n",
    "        \n",
    "        self.quant    = torch.quantization.QuantStub(my_qconfig)\n",
    "        self.sq1      = LinearReLU(784,1500)\n",
    "        \n",
    "        self.sq2      = LinearReLU(1500,750)\n",
    "        \n",
    "        self.sq3      = LinearReLU(750,300)\n",
    "        \n",
    "        self.fc_out   = nn.Linear(300, 10, bias=False)\n",
    "        \n",
    "        self.dequant  = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = self.quant(x)\n",
    "        \n",
    "        x = F.relu(self.sq1(x))\n",
    "        x = F.relu(self.sq2(x))\n",
    "        x = F.relu(self.sq3(x))\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        x = self.dequant(x)\n",
    "        \n",
    "        return x                        # No need for F.log_softmax() as Cross Entropy loss does that implicitly\n",
    "    \n",
    "    def fuse_model(self):\n",
    "        for m in self.modules():\n",
    "            if type(m) == LinearReLU:\n",
    "                torch.quantization.fuse_modules(m, ['0', '1'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to just implement the forward pass for observer calibration\n",
    "\n",
    "def calibrate(model, criterion, data_loader, n_eval=150):\n",
    "    model.eval()\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            count += 1\n",
    "            if count >= n_eval:\n",
    "                return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.68940 \tValidation Loss: 0.50005\n",
      "Validation loss has decreased (inf --> 0.50005).  Model saved!\n",
      "\n",
      "Epoch: 2 \tTraining Loss: 0.45383 \tValidation Loss: 0.42053\n",
      "Validation loss has decreased (0.50005 --> 0.42053).  Model saved!\n",
      "\n",
      "Epoch: 3 \tTraining Loss: 0.41588 \tValidation Loss: 0.44423\n",
      "\n",
      "Epoch: 4 \tTraining Loss: 0.40616 \tValidation Loss: 0.43227\n",
      "\n",
      "Epoch: 5 \tTraining Loss: 0.39755 \tValidation Loss: 0.38668\n",
      "Validation loss has decreased (0.42053 --> 0.38668).  Model saved!\n",
      "\n",
      "85.41\n"
     ]
    }
   ],
   "source": [
    "# Train the model with the observers in place\n",
    "\n",
    "model_quant = FMNIST_quant()\n",
    "\n",
    "# Defining the Loss function and the Optimization function \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model_quant.parameters(), lr=0.01)\n",
    "\n",
    "model_quant, model_quant_accuracy = main(model_quant)\n",
    "print(model_quant_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the model\n",
    "\n",
    "# model_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, dtype=torch.quint8), weight=functools.partial(functools.partial(<class 'torch.quantization.observer.PerChannelMinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_channel_symmetric), dtype=torch.qint8))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FMNIST_quant(\n",
       "  (quant): Quantize(scale=tensor([0.0039]), zero_point=tensor([0]), dtype=torch.quint8)\n",
       "  (sq1): LinearReLU(\n",
       "    (0): QuantizedLinearReLU(in_features=784, out_features=1500, scale=0.12596023082733154, zero_point=0, qscheme=torch.per_channel_affine)\n",
       "    (1): Identity()\n",
       "  )\n",
       "  (sq2): LinearReLU(\n",
       "    (0): QuantizedLinearReLU(in_features=1500, out_features=750, scale=0.15319477021694183, zero_point=0, qscheme=torch.per_channel_affine)\n",
       "    (1): Identity()\n",
       "  )\n",
       "  (sq3): LinearReLU(\n",
       "    (0): QuantizedLinearReLU(in_features=750, out_features=300, scale=0.5363470315933228, zero_point=0, qscheme=torch.per_channel_affine)\n",
       "    (1): Identity()\n",
       "  )\n",
       "  (fc_out): QuantizedLinear(in_features=300, out_features=10, scale=1.0589793920516968, zero_point=198, qscheme=torch.per_channel_affine)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model_with_stubs.load_state_dict(torch.load('model_q.pt'))\n",
    "model_quant.eval()\n",
    "\n",
    "model_quant.fuse_model()\n",
    "\n",
    "# Set the qconfig - fbgemm for x86 and qnnpack for ARM qnnpack\n",
    "model_quant.qconfig = QConfig(activation=MinMaxObserver.with_args(dtype=torch.quint8),\n",
    "                        weight=default_per_channel_weight_observer.with_args(dtype=torch.qint8))\n",
    "print(model_quant.qconfig)\n",
    "torch.backends.quantized.engine = 'fbgemm'\n",
    "\n",
    "# Insert observers, calibrate the model and collect statistics\n",
    "torch.quantization.prepare(model_quant, inplace=True)\n",
    "\n",
    "calibrate(model_quant,criterion,testloader)\n",
    "\n",
    "# Convert to the quantized version\n",
    "torch.quantization.convert(model_quant, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size (MB): 2.573233\n",
      "Size (MB): 10.116807\n"
     ]
    }
   ],
   "source": [
    "# Compare the model size\n",
    "\n",
    "print_size_of_model(model_quant)  # quantized model\n",
    "print_size_of_model(model)        # original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the static quantized model : 84.44%\n",
      "Accuracy of the original model         : 85.41%\n"
     ]
    }
   ],
   "source": [
    "# Compare the test accuracies\n",
    "\n",
    "static_quantized_accuracy = test_accuracy(model_quant, testloader, criterion)\n",
    "\n",
    "print(\"Accuracy of the static quantized model : {}%\".format(static_quantized_accuracy))\n",
    "print(\"Accuracy of the original model         : {}%\".format(model_quant_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  34,   46, -100,  ...,  -64,  -33,  -27],\n",
       "        [  21,   -3,  -12,  ...,  -76,  -61,   54],\n",
       "        [ -19,   13,  -80,  ...,  -68,  -68,   -3],\n",
       "        ...,\n",
       "        [ -23,  -32,    8,  ...,  -44,   -4,  -31],\n",
       "        [ -35,   54,  -75,  ...,  -79,  -72,   24],\n",
       "        [  11,    9,  -24,  ...,  -58,  -62,  -32]], dtype=torch.int8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the quantized weights\n",
    "\n",
    "model_quant.sq1[0].weight().int_repr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quant.scale\n",
      "tensor([0.0039])\n",
      "\n",
      "quant.zero_point\n",
      "tensor([0])\n",
      "\n",
      "sq1.0.scale\n",
      "tensor(0.1260)\n",
      "\n",
      "sq1.0.zero_point\n",
      "tensor(0)\n",
      "\n",
      "sq1.0._packed_params.weight\n",
      "tensor([[ 0.0304,  0.0412, -0.0895,  ..., -0.0573, -0.0295, -0.0242],\n",
      "        [ 0.0199, -0.0028, -0.0114,  ..., -0.0720, -0.0578,  0.0511],\n",
      "        [-0.0178,  0.0122, -0.0750,  ..., -0.0637, -0.0637, -0.0028],\n",
      "        ...,\n",
      "        [-0.0260, -0.0362,  0.0090,  ..., -0.0497, -0.0045, -0.0351],\n",
      "        [-0.0327,  0.0504, -0.0701,  ..., -0.0738, -0.0673,  0.0224],\n",
      "        [ 0.0101,  0.0083, -0.0221,  ..., -0.0535, -0.0571, -0.0295]],\n",
      "       size=(1500, 784), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0009, 0.0009, 0.0009,  ..., 0.0011, 0.0009, 0.0009],\n",
      "       dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0,  ..., 0, 0, 0]), axis=0)\n",
      "\n",
      "sq1.0._packed_params.bias\n",
      "None\n",
      "\n",
      "sq1.0._packed_params.dtype\n",
      "torch.qint8\n",
      "\n",
      "sq2.0.scale\n",
      "tensor(0.1532)\n",
      "\n",
      "sq2.0.zero_point\n",
      "tensor(0)\n",
      "\n",
      "sq2.0._packed_params.weight\n",
      "tensor([[ 0.0359, -0.1033,  0.0881,  ...,  0.0348,  0.0685,  0.0207],\n",
      "        [-0.0292, -0.0530, -0.0420,  ..., -0.0822, -0.0548, -0.0603],\n",
      "        [-0.0581, -0.0775, -0.0581,  ...,  0.1065, -0.0387, -0.0775],\n",
      "        ...,\n",
      "        [ 0.0823, -0.0529, -0.0558,  ..., -0.0617, -0.0529, -0.0617],\n",
      "        [-0.0501, -0.0421, -0.0421,  ..., -0.0561, -0.0642, -0.0401],\n",
      "        [-0.0429,  0.0686, -0.0300,  ..., -0.0686, -0.0300, -0.0514]],\n",
      "       size=(750, 1500), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0011, 0.0018, 0.0097, 0.0073, 0.0059, 0.0047, 0.0039, 0.0043, 0.0084,\n",
      "        0.0024, 0.0011, 0.0032, 0.0020, 0.0047, 0.0022, 0.0082, 0.0038, 0.0034,\n",
      "        0.0058, 0.0025, 0.0061, 0.0021, 0.0020, 0.0056, 0.0024, 0.0088, 0.0064,\n",
      "        0.0048, 0.0017, 0.0023, 0.0068, 0.0101, 0.0055, 0.0020, 0.0015, 0.0017,\n",
      "        0.0027, 0.0015, 0.0029, 0.0022, 0.0023, 0.0025, 0.0066, 0.0096, 0.0042,\n",
      "        0.0012, 0.0068, 0.0091, 0.0049, 0.0029, 0.0044, 0.0013, 0.0101, 0.0052,\n",
      "        0.0039, 0.0062, 0.0074, 0.0013, 0.0022, 0.0044, 0.0043, 0.0077, 0.0037,\n",
      "        0.0033, 0.0059, 0.0026, 0.0041, 0.0070, 0.0055, 0.0052, 0.0091, 0.0032,\n",
      "        0.0071, 0.0073, 0.0012, 0.0020, 0.0018, 0.0123, 0.0050, 0.0039, 0.0047,\n",
      "        0.0021, 0.0044, 0.0064, 0.0016, 0.0026, 0.0035, 0.0019, 0.0033, 0.0041,\n",
      "        0.0039, 0.0039, 0.0031, 0.0044, 0.0025, 0.0014, 0.0070, 0.0012, 0.0025,\n",
      "        0.0024, 0.0012, 0.0018, 0.0060, 0.0016, 0.0018, 0.0048, 0.0080, 0.0009,\n",
      "        0.0042, 0.0020, 0.0085, 0.0023, 0.0081, 0.0028, 0.0066, 0.0036, 0.0061,\n",
      "        0.0009, 0.0037, 0.0041, 0.0059, 0.0016, 0.0008, 0.0024, 0.0048, 0.0032,\n",
      "        0.0057, 0.0089, 0.0013, 0.0020, 0.0045, 0.0029, 0.0058, 0.0027, 0.0030,\n",
      "        0.0063, 0.0156, 0.0046, 0.0043, 0.0070, 0.0024, 0.0048, 0.0066, 0.0057,\n",
      "        0.0031, 0.0020, 0.0098, 0.0020, 0.0032, 0.0008, 0.0027, 0.0017, 0.0047,\n",
      "        0.0047, 0.0047, 0.0020, 0.0008, 0.0023, 0.0019, 0.0047, 0.0029, 0.0041,\n",
      "        0.0059, 0.0033, 0.0029, 0.0054, 0.0029, 0.0024, 0.0040, 0.0039, 0.0035,\n",
      "        0.0038, 0.0020, 0.0073, 0.0026, 0.0054, 0.0045, 0.0059, 0.0022, 0.0031,\n",
      "        0.0018, 0.0022, 0.0049, 0.0028, 0.0017, 0.0053, 0.0034, 0.0031, 0.0020,\n",
      "        0.0028, 0.0058, 0.0060, 0.0060, 0.0030, 0.0063, 0.0037, 0.0019, 0.0011,\n",
      "        0.0024, 0.0060, 0.0038, 0.0025, 0.0044, 0.0033, 0.0055, 0.0031, 0.0034,\n",
      "        0.0070, 0.0019, 0.0066, 0.0048, 0.0032, 0.0034, 0.0045, 0.0073, 0.0038,\n",
      "        0.0024, 0.0026, 0.0023, 0.0051, 0.0051, 0.0029, 0.0017, 0.0037, 0.0046,\n",
      "        0.0042, 0.0018, 0.0051, 0.0014, 0.0050, 0.0069, 0.0044, 0.0044, 0.0033,\n",
      "        0.0053, 0.0077, 0.0029, 0.0163, 0.0044, 0.0029, 0.0039, 0.0040, 0.0057,\n",
      "        0.0024, 0.0035, 0.0130, 0.0015, 0.0027, 0.0017, 0.0021, 0.0045, 0.0034,\n",
      "        0.0009, 0.0068, 0.0045, 0.0017, 0.0073, 0.0052, 0.0046, 0.0021, 0.0056,\n",
      "        0.0042, 0.0065, 0.0062, 0.0022, 0.0010, 0.0016, 0.0067, 0.0031, 0.0092,\n",
      "        0.0087, 0.0050, 0.0062, 0.0026, 0.0033, 0.0038, 0.0053, 0.0036, 0.0061,\n",
      "        0.0060, 0.0054, 0.0047, 0.0014, 0.0036, 0.0053, 0.0031, 0.0026, 0.0014,\n",
      "        0.0023, 0.0061, 0.0055, 0.0019, 0.0030, 0.0040, 0.0035, 0.0033, 0.0041,\n",
      "        0.0033, 0.0043, 0.0047, 0.0026, 0.0045, 0.0036, 0.0032, 0.0045, 0.0016,\n",
      "        0.0034, 0.0018, 0.0024, 0.0022, 0.0015, 0.0033, 0.0052, 0.0073, 0.0047,\n",
      "        0.0026, 0.0010, 0.0065, 0.0047, 0.0048, 0.0083, 0.0070, 0.0047, 0.0067,\n",
      "        0.0056, 0.0031, 0.0017, 0.0038, 0.0047, 0.0021, 0.0031, 0.0019, 0.0032,\n",
      "        0.0021, 0.0018, 0.0071, 0.0036, 0.0047, 0.0041, 0.0014, 0.0074, 0.0009,\n",
      "        0.0048, 0.0013, 0.0064, 0.0030, 0.0094, 0.0017, 0.0053, 0.0064, 0.0032,\n",
      "        0.0098, 0.0045, 0.0024, 0.0025, 0.0024, 0.0020, 0.0023, 0.0048, 0.0021,\n",
      "        0.0024, 0.0027, 0.0077, 0.0020, 0.0011, 0.0049, 0.0052, 0.0070, 0.0012,\n",
      "        0.0033, 0.0034, 0.0025, 0.0045, 0.0038, 0.0039, 0.0079, 0.0018, 0.0054,\n",
      "        0.0157, 0.0021, 0.0053, 0.0033, 0.0090, 0.0047, 0.0033, 0.0065, 0.0025,\n",
      "        0.0048, 0.0066, 0.0026, 0.0018, 0.0028, 0.0018, 0.0038, 0.0042, 0.0076,\n",
      "        0.0046, 0.0048, 0.0086, 0.0034, 0.0150, 0.0027, 0.0057, 0.0018, 0.0026,\n",
      "        0.0041, 0.0008, 0.0042, 0.0058, 0.0040, 0.0030, 0.0038, 0.0032, 0.0016,\n",
      "        0.0054, 0.0029, 0.0056, 0.0027, 0.0069, 0.0049, 0.0060, 0.0017, 0.0021,\n",
      "        0.0063, 0.0015, 0.0042, 0.0088, 0.0105, 0.0082, 0.0027, 0.0033, 0.0049,\n",
      "        0.0046, 0.0020, 0.0010, 0.0049, 0.0066, 0.0030, 0.0021, 0.0038, 0.0023,\n",
      "        0.0030, 0.0022, 0.0022, 0.0015, 0.0064, 0.0037, 0.0032, 0.0017, 0.0017,\n",
      "        0.0071, 0.0048, 0.0053, 0.0034, 0.0071, 0.0052, 0.0049, 0.0010, 0.0058,\n",
      "        0.0052, 0.0018, 0.0011, 0.0052, 0.0029, 0.0069, 0.0047, 0.0021, 0.0012,\n",
      "        0.0056, 0.0010, 0.0037, 0.0060, 0.0026, 0.0028, 0.0016, 0.0008, 0.0015,\n",
      "        0.0027, 0.0039, 0.0049, 0.0013, 0.0033, 0.0023, 0.0034, 0.0075, 0.0050,\n",
      "        0.0061, 0.0016, 0.0015, 0.0051, 0.0038, 0.0047, 0.0022, 0.0058, 0.0124,\n",
      "        0.0009, 0.0025, 0.0023, 0.0017, 0.0024, 0.0011, 0.0067, 0.0027, 0.0082,\n",
      "        0.0074, 0.0047, 0.0010, 0.0023, 0.0015, 0.0077, 0.0014, 0.0074, 0.0020,\n",
      "        0.0050, 0.0062, 0.0047, 0.0074, 0.0065, 0.0017, 0.0065, 0.0064, 0.0055,\n",
      "        0.0049, 0.0016, 0.0012, 0.0047, 0.0036, 0.0014, 0.0020, 0.0032, 0.0021,\n",
      "        0.0060, 0.0026, 0.0034, 0.0124, 0.0017, 0.0047, 0.0055, 0.0010, 0.0050,\n",
      "        0.0030, 0.0047, 0.0022, 0.0019, 0.0019, 0.0048, 0.0013, 0.0042, 0.0028,\n",
      "        0.0035, 0.0019, 0.0016, 0.0077, 0.0084, 0.0034, 0.0020, 0.0063, 0.0067,\n",
      "        0.0018, 0.0021, 0.0014, 0.0049, 0.0019, 0.0017, 0.0039, 0.0025, 0.0095,\n",
      "        0.0028, 0.0025, 0.0058, 0.0100, 0.0029, 0.0061, 0.0032, 0.0024, 0.0033,\n",
      "        0.0019, 0.0098, 0.0029, 0.0012, 0.0020, 0.0046, 0.0059, 0.0021, 0.0030,\n",
      "        0.0050, 0.0043, 0.0037, 0.0048, 0.0059, 0.0036, 0.0014, 0.0015, 0.0044,\n",
      "        0.0039, 0.0023, 0.0034, 0.0030, 0.0019, 0.0038, 0.0012, 0.0028, 0.0008,\n",
      "        0.0066, 0.0028, 0.0049, 0.0022, 0.0022, 0.0020, 0.0040, 0.0046, 0.0058,\n",
      "        0.0016, 0.0035, 0.0027, 0.0058, 0.0064, 0.0046, 0.0035, 0.0065, 0.0077,\n",
      "        0.0017, 0.0091, 0.0022, 0.0051, 0.0031, 0.0010, 0.0021, 0.0061, 0.0046,\n",
      "        0.0040, 0.0019, 0.0016, 0.0025, 0.0086, 0.0046, 0.0012, 0.0034, 0.0014,\n",
      "        0.0066, 0.0062, 0.0042, 0.0032, 0.0034, 0.0038, 0.0054, 0.0038, 0.0026,\n",
      "        0.0018, 0.0060, 0.0024, 0.0038, 0.0022, 0.0019, 0.0020, 0.0056, 0.0024,\n",
      "        0.0029, 0.0029, 0.0034, 0.0094, 0.0024, 0.0037, 0.0011, 0.0027, 0.0041,\n",
      "        0.0097, 0.0050, 0.0020, 0.0067, 0.0080, 0.0032, 0.0052, 0.0090, 0.0012,\n",
      "        0.0080, 0.0023, 0.0069, 0.0028, 0.0039, 0.0010, 0.0022, 0.0054, 0.0079,\n",
      "        0.0018, 0.0026, 0.0054, 0.0035, 0.0014, 0.0044, 0.0041, 0.0071, 0.0039,\n",
      "        0.0032, 0.0029, 0.0011, 0.0022, 0.0018, 0.0048, 0.0010, 0.0039, 0.0052,\n",
      "        0.0029, 0.0061, 0.0041, 0.0034, 0.0027, 0.0030, 0.0058, 0.0052, 0.0023,\n",
      "        0.0063, 0.0043, 0.0067, 0.0021, 0.0082, 0.0015, 0.0034, 0.0041, 0.0035,\n",
      "        0.0035, 0.0017, 0.0068, 0.0021, 0.0049, 0.0012, 0.0044, 0.0029, 0.0070,\n",
      "        0.0053, 0.0020, 0.0100, 0.0026, 0.0027, 0.0087, 0.0049, 0.0055, 0.0012,\n",
      "        0.0014, 0.0071, 0.0011, 0.0025, 0.0028, 0.0027, 0.0024, 0.0061, 0.0023,\n",
      "        0.0029, 0.0020, 0.0043], dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0]),\n",
      "       axis=0)\n",
      "\n",
      "sq2.0._packed_params.bias\n",
      "None\n",
      "\n",
      "sq2.0._packed_params.dtype\n",
      "torch.qint8\n",
      "\n",
      "sq3.0.scale\n",
      "tensor(0.5363)\n",
      "\n",
      "sq3.0.zero_point\n",
      "tensor(0)\n",
      "\n",
      "sq3.0._packed_params.weight\n",
      "tensor([[-0.0467, -0.0389, -0.0855,  ..., -0.1244, -0.0311, -0.0544],\n",
      "        [-0.0484, -0.0580, -0.2031,  ..., -0.2128, -0.0677, -0.0484],\n",
      "        [-0.0576, -0.0432,  0.1295,  ..., -0.0288, -0.0576, -0.0432],\n",
      "        ...,\n",
      "        [ 0.1160,  0.2256, -0.1869,  ..., -0.1096,  0.2578, -0.2514],\n",
      "        [-0.0911, -0.0561, -0.0514,  ..., -0.0631, -0.0374, -0.0748],\n",
      "        [-0.0811, -0.0325,  0.0000,  ..., -0.0892, -0.0406, -0.0568]],\n",
      "       size=(300, 750), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0078, 0.0097, 0.0144, 0.0051, 0.0008, 0.0063, 0.0055, 0.0061, 0.0008,\n",
      "        0.0054, 0.0072, 0.0056, 0.0084, 0.0063, 0.0024, 0.0086, 0.0022, 0.0074,\n",
      "        0.0053, 0.0054, 0.0065, 0.0091, 0.0030, 0.0025, 0.0067, 0.0062, 0.0067,\n",
      "        0.0050, 0.0052, 0.0076, 0.0028, 0.0063, 0.0025, 0.0020, 0.0048, 0.0090,\n",
      "        0.0086, 0.0048, 0.0055, 0.0048, 0.0083, 0.0063, 0.0124, 0.0057, 0.0052,\n",
      "        0.0071, 0.0077, 0.0066, 0.0054, 0.0075, 0.0055, 0.0044, 0.0060, 0.0085,\n",
      "        0.0008, 0.0030, 0.0098, 0.0068, 0.0054, 0.0029, 0.0032, 0.0054, 0.0034,\n",
      "        0.0022, 0.0068, 0.0074, 0.0069, 0.0051, 0.0067, 0.0029, 0.0064, 0.0047,\n",
      "        0.0085, 0.0076, 0.0017, 0.0085, 0.0055, 0.0040, 0.0026, 0.0085, 0.0071,\n",
      "        0.0090, 0.0076, 0.0052, 0.0019, 0.0053, 0.0118, 0.0060, 0.0037, 0.0061,\n",
      "        0.0127, 0.0095, 0.0053, 0.0018, 0.0046, 0.0052, 0.0069, 0.0048, 0.0070,\n",
      "        0.0044, 0.0053, 0.0101, 0.0058, 0.0084, 0.0061, 0.0117, 0.0045, 0.0030,\n",
      "        0.0025, 0.0053, 0.0066, 0.0118, 0.0048, 0.0164, 0.0049, 0.0037, 0.0054,\n",
      "        0.0067, 0.0080, 0.0052, 0.0078, 0.0024, 0.0052, 0.0072, 0.0104, 0.0057,\n",
      "        0.0059, 0.0088, 0.0131, 0.0075, 0.0058, 0.0102, 0.0080, 0.0072, 0.0063,\n",
      "        0.0029, 0.0096, 0.0023, 0.0018, 0.0022, 0.0017, 0.0088, 0.0054, 0.0064,\n",
      "        0.0025, 0.0063, 0.0095, 0.0029, 0.0058, 0.0064, 0.0017, 0.0013, 0.0040,\n",
      "        0.0026, 0.0057, 0.0100, 0.0059, 0.0094, 0.0060, 0.0073, 0.0099, 0.0045,\n",
      "        0.0051, 0.0075, 0.0011, 0.0034, 0.0046, 0.0069, 0.0060, 0.0045, 0.0035,\n",
      "        0.0068, 0.0102, 0.0008, 0.0049, 0.0073, 0.0106, 0.0079, 0.0088, 0.0030,\n",
      "        0.0101, 0.0047, 0.0010, 0.0081, 0.0031, 0.0076, 0.0056, 0.0071, 0.0087,\n",
      "        0.0057, 0.0059, 0.0122, 0.0069, 0.0089, 0.0103, 0.0028, 0.0059, 0.0060,\n",
      "        0.0062, 0.0060, 0.0034, 0.0063, 0.0018, 0.0029, 0.0028, 0.0069, 0.0029,\n",
      "        0.0049, 0.0051, 0.0053, 0.0008, 0.0071, 0.0038, 0.0027, 0.0039, 0.0069,\n",
      "        0.0016, 0.0023, 0.0055, 0.0066, 0.0061, 0.0025, 0.0043, 0.0039, 0.0077,\n",
      "        0.0092, 0.0059, 0.0027, 0.0059, 0.0089, 0.0093, 0.0052, 0.0041, 0.0026,\n",
      "        0.0061, 0.0052, 0.0060, 0.0101, 0.0033, 0.0054, 0.0065, 0.0070, 0.0078,\n",
      "        0.0073, 0.0059, 0.0079, 0.0040, 0.0059, 0.0070, 0.0044, 0.0064, 0.0008,\n",
      "        0.0049, 0.0098, 0.0020, 0.0063, 0.0039, 0.0047, 0.0027, 0.0069, 0.0072,\n",
      "        0.0051, 0.0055, 0.0054, 0.0016, 0.0102, 0.0046, 0.0099, 0.0043, 0.0035,\n",
      "        0.0066, 0.0075, 0.0054, 0.0172, 0.0072, 0.0044, 0.0037, 0.0064, 0.0067,\n",
      "        0.0026, 0.0070, 0.0031, 0.0053, 0.0064, 0.0059, 0.0106, 0.0051, 0.0077,\n",
      "        0.0067, 0.0040, 0.0106, 0.0061, 0.0026, 0.0070, 0.0078, 0.0008, 0.0093,\n",
      "        0.0064, 0.0023, 0.0081], dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
      "       axis=0)\n",
      "\n",
      "sq3.0._packed_params.bias\n",
      "None\n",
      "\n",
      "sq3.0._packed_params.dtype\n",
      "torch.qint8\n",
      "\n",
      "fc_out.scale\n",
      "tensor(1.0590)\n",
      "\n",
      "fc_out.zero_point\n",
      "tensor(198)\n",
      "\n",
      "fc_out._packed_params.weight\n",
      "tensor([[ 0.0550,  0.0688,  0.0275,  ..., -0.1582, -0.2890,  0.0963],\n",
      "        [ 0.0467,  0.1518, -0.0117,  ..., -0.1285, -0.0818, -0.0117],\n",
      "        [-0.0237, -0.0119, -0.0237,  ...,  0.1069, -0.2137, -0.0119],\n",
      "        ...,\n",
      "        [ 0.0572, -0.0143,  0.0572,  ..., -0.1429,  0.0143,  0.1143],\n",
      "        [ 0.0617,  0.0740,  0.0370,  ..., -0.0123,  0.0987,  0.0493],\n",
      "        [ 0.0598,  0.0837,  0.0359,  ..., -0.0837,  0.0120,  0.1076]],\n",
      "       size=(10, 300), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0069, 0.0117, 0.0119, 0.0112, 0.0131, 0.0102, 0.0107, 0.0143, 0.0123,\n",
      "        0.0120], dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), axis=0)\n",
      "\n",
      "fc_out._packed_params.bias\n",
      "None\n",
      "\n",
      "fc_out._packed_params.dtype\n",
      "torch.qint8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the stat_dict of the model\n",
    "\n",
    "for key,val in model_quant.state_dict().items():\n",
    "    print(key)\n",
    "    print(val)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
